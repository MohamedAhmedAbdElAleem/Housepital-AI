{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1: Binary Classification - Testing & Inference\n",
    "\n",
    "This notebook allows you to test the trained **Binary Classification Model (Healthy vs Wound)**.\n",
    "\n",
    "**Supported Modes:**\n",
    "1. **Single Image**: Predict on a specific image path.\n",
    "2. **Folder**: Predict on all images in a directory.\n",
    "3. **Test Set**: Evaluate metrics (Accuracy, F1, AUC) on `test.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Housepital-AI\\Housepital-AI\\venv_gpu\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import timm\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Config\n",
    "CONFIG = {\n",
    "    \"img_size\": 224,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_workers\": 0,\n",
    "    \"model_name\": \"tf_efficientnet_b0\",\n",
    "    \"model_path\": \"../models/stage1_binary/best_model_fold_0.pth\",\n",
    "    \"test_csv\": \"../data/loaders/test.csv\",\n",
    "    \"root_dir\": \"../\"\n",
    "}\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded Successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Helper Classes ---\n",
    "\n",
    "class WoundDatasetDF(Dataset):\n",
    "    def __init__(self, df, root_dir=None, transform=None, binary_mode=False):\n",
    "        self.annotations = df\n",
    "        self.root_dir = Path(root_dir) if root_dir else Path(\".\")\n",
    "        self.transform = transform\n",
    "        self.binary_mode = binary_mode\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.annotations.iloc[index]\n",
    "        rel_path = row['path']\n",
    "        rel_path = str(rel_path).replace('\\\\', os.sep).replace('/', os.sep)\n",
    "        \n",
    "        img_path = self.root_dir / rel_path\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except:\n",
    "             # Fallback\n",
    "            if rel_path.startswith(\"..\"):\n",
    "                 img_path = self.root_dir / rel_path[3:]\n",
    "            try:\n",
    "                image = Image.open(img_path).convert(\"RGB\")\n",
    "            except:\n",
    "                image = Image.new('RGB', (224, 224), color='black')\n",
    "            \n",
    "        # Handle missing label for inference mode\n",
    "        if 'label' in row:\n",
    "            label_str = row['label']\n",
    "            if self.binary_mode:\n",
    "                label = 0 if str(label_str).lower() == 'healthy' else 1\n",
    "            else:\n",
    "                label = 0 # Default placeholder\n",
    "        else:\n",
    "            label = -1 # Unknown\n",
    "            \n",
    "        if self.transform:\n",
    "            image = np.array(image)\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "            \n",
    "        return image, torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "def get_transforms():\n",
    "    return A.Compose([\n",
    "        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "def load_model():\n",
    "    model = timm.create_model(CONFIG['model_name'], pretrained=False, num_classes=1)\n",
    "    model.load_state_dict(torch.load(CONFIG['model_path'], map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Load Model Once\n",
    "model = load_model()\n",
    "transforms = get_transforms()\n",
    "print(\"Model Loaded Successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Single Image Inference\n",
    "Change `image_path` to test a specific file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('Healthy', 0.8275864124298096)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def predict_image(image_path, model, transforms, device):\n",
    "#     try:\n",
    "#         image = Image.open(image_path).convert(\"RGB\")\n",
    "#         img_np = np.array(image)\n",
    "#         aug = transforms(image=img_np)\n",
    "#         img_tensor = aug['image'].unsqueeze(0).to(device)\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             output = model(img_tensor)\n",
    "#             prob = torch.sigmoid(output).item()\n",
    "            \n",
    "#         pred_label = \"Wound\" if prob > 0.5 else \"Healthy\"\n",
    "#         confidence = prob if prob > 0.5 else 1 - prob\n",
    "        \n",
    "#         plt.imshow(image)\n",
    "#         plt.title(f\"Pred: {pred_label} ({confidence:.2%})\")\n",
    "#         plt.axis('off')\n",
    "#         plt.show()\n",
    "        \n",
    "#         return pred_label, confidence\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error: {e}\")\n",
    "\n",
    "# # Example Usage - Replace with absolute path if needed\n",
    "# predict_image(\"../data/raw/type_classification/cut/cut_01525.jpg\", model, transforms, device)\n",
    "# predict_image(\"E:\\\\download\\\\logo.png\", model, transforms, device)\n",
    "# predict_image(\"F:\\\\Housepital-AI\\\\Housepital-AI\\\\dfu_dataset\\\\Wound Multitask Data Set\\\\Fresh Images\\\\male (346).jpg\", model, transforms, device)\n",
    "# predict_image(\"E:\\\\download\\\\A 3D character of a small boy with a friendly and supportive demeanor, similar to Samir the Study Buddy. The character should have short, tidy hair with a playful tuft, bright and expressive eyes, and casual clothing.png\", model, transforms, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization & Splitting Verification\n",
    "\n",
    "This notebook verifies the output of `split_data.py`. We check:\n",
    "1. **Test Set Distribution**: Ensuring the 15% hold-out set is stratified.\n",
    "2. **Cross-Validation Folds**: Ensuring the 5 folds are balanced and representative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Set Evaluation\n",
    "Evaluate on `test.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [00:51<00:00,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Healthy       0.95      0.97      0.96       600\n",
      "       Wound       0.99      0.99      0.99      2908\n",
      "\n",
      "    accuracy                           0.98      3508\n",
      "   macro avg       0.97      0.98      0.97      3508\n",
      "weighted avg       0.99      0.98      0.98      3508\n",
      "\n",
      "ROC AUC: 0.9962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(CONFIG['test_csv']):\n",
    "    df_test = pd.read_csv(CONFIG['test_csv'])\n",
    "    test_ds = WoundDatasetDF(df_test, root_dir=CONFIG['root_dir'], transform=transforms, binary_mode=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=0, pin_memory=False)\n",
    "    \n",
    "    preds_all = []\n",
    "    targets_all = []\n",
    "    \n",
    "    print(\"Running Evaluation...\")\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader):\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            preds_all.extend(torch.sigmoid(outputs).cpu().numpy())\n",
    "            targets_all.extend(labels.cpu().numpy())\n",
    "            \n",
    "    preds_binary = (np.array(preds_all) > 0.5).astype(int)\n",
    "    targets_all = np.array(targets_all)\n",
    "    \n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(targets_all, preds_binary, target_names=['Healthy', 'Wound']))\n",
    "    print(f\"ROC AUC: {roc_auc_score(targets_all, preds_all):.4f}\")\n",
    "else:\n",
    "    print(\"Test CSV not found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
