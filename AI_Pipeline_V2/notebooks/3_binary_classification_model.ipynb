{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1: Binary Classification Model (Healthy vs Wound)\n",
    "\n",
    "This notebook trains the first stage of the cascade: **The Gatekeeper**.\n",
    "Its goal is to filter out healthy/irrelevant images so the downstream models only see actual wounds.\n",
    "\n",
    "**Strategy:**\n",
    "- **Model**: EfficientNet-B0 (Pretrained on ImageNet)\n",
    "- **Data**: All Wound Classes (Positives) vs Healthy (Negatives)\n",
    "- **Loss**: BCEWithLogitsLoss\n",
    "- **Validation**: 5-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import timm\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "\n",
    "# Add src to path for imports\n",
    "sys.path.append(\"../src\")\n",
    "from dataset import WoundDataset\n",
    "\n",
    "# Config\n",
    "CONFIG = {\n",
    "    \"seed\": 42,\n",
    "    \"img_size\": 224,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_workers\": 0, # Changed from 4 to 0 for Windows compatibility\n",
    "    \"epochs\": 5,\n",
    "    \"lr\": 1e-3,\n",
    "    \"model_name\": \"tf_efficientnet_b2\",\n",
    "    \"model_dir\": \"../models/stage1_binary/\",\n",
    "    \"data_csv\": \"../data/loaders/train_folds.csv\",\n",
    "    \"root_dir\": \"../\"\n",
    "}\n",
    "\n",
    "os.makedirs(CONFIG[\"model_dir\"], exist_ok=True)\n",
    "\n",
    "# Set Seed\n",
    "def seed_everything(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(CONFIG['seed'])\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "We use `Fold 0` for validation in this first run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms\n",
    "train_transforms = A.Compose([\n",
    "    A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.Rotate(limit=30, p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transforms = A.Compose([\n",
    "    A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# Load DF\n",
    "df = pd.read_csv(CONFIG['data_csv'])\n",
    "\n",
    "# Split by Fold\n",
    "FOLD = 0\n",
    "train_df = df[df['fold'] != FOLD].reset_index(drop=True)\n",
    "val_df = df[df['fold'] == FOLD].reset_index(drop=True)\n",
    "\n",
    "# Create Datasets (Binary Mode = True)\n",
    "train_dataset = WoundDataset(\n",
    "    csv_file=CONFIG['data_csv'], \n",
    "    root_dir=CONFIG['root_dir'], \n",
    "    transform=train_transforms,\n",
    "    binary_mode=True\n",
    ")\n",
    "\n",
    "print(f\"Train Samples: {len(train_df)} | Val Samples: {len(val_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Wrapper for DataFrame\n",
    "class WoundDatasetDF(Dataset):\n",
    "    def __init__(self, df, root_dir=None, transform=None, binary_mode=False):\n",
    "        self.annotations = df\n",
    "        self.root_dir = Path(root_dir) if root_dir else Path(\".\")\n",
    "        self.transform = transform\n",
    "        self.binary_mode = binary_mode\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.annotations.iloc[index]\n",
    "        rel_path = row['path']\n",
    "        # Fix paths\n",
    "        rel_path = str(rel_path).replace('\\\\', os.sep).replace('/', os.sep)\n",
    "        if rel_path.startswith(\"..\"):\n",
    "            # handle relative paths if needed, but usually fine\n",
    "            pass\n",
    "        \n",
    "        img_path = self.root_dir / rel_path\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except:\n",
    "            # Fallback logic\n",
    "            if rel_path.startswith(\"..\"):\n",
    "                 img_path = self.root_dir / rel_path[3:]\n",
    "            try:\n",
    "                image = Image.open(img_path).convert(\"RGB\")\n",
    "            except:\n",
    "                # Create a black image as desperate fallback to not crash training\n",
    "                print(f\"Warning: Could not open {img_path}, using black image.\")\n",
    "                image = Image.new('RGB', (224, 224), color='black')\n",
    "            \n",
    "        label_str = row['label'] \n",
    "        \n",
    "        if self.binary_mode:\n",
    "            label = 0 if label_str.lower() == 'healthy' else 1\n",
    "        else:\n",
    "            pass\n",
    "            \n",
    "        if self.transform:\n",
    "            image = np.array(image)\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "            \n",
    "        return image, torch.tensor(label, dtype=torch.float32) # Float for BCEWithLogits\n",
    "\n",
    "# Init datasets\n",
    "train_ds = WoundDatasetDF(train_df, root_dir=CONFIG['root_dir'], transform=train_transforms, binary_mode=True)\n",
    "val_ds = WoundDatasetDF(val_df, root_dir=CONFIG['root_dir'], transform=val_transforms, binary_mode=True)\n",
    "\n",
    "# No pin_memory for Windows stability\n",
    "train_loader = DataLoader(train_ds, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=CONFIG['num_workers'], pin_memory=False)\n",
    "val_loader = DataLoader(val_ds, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=CONFIG['num_workers'], pin_memory=False)\n",
    "\n",
    "# Test Batch\n",
    "img, lab = next(iter(train_loader))\n",
    "print(f\"Batch Shape: {img.shape}, Label Shape: {lab.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Factory & Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name, num_classes=1, pretrained=True):\n",
    "    model = timm.create_model(model_name, pretrained=pretrained, num_classes=num_classes)\n",
    "    return model\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device, scaler=None):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    preds_all = []\n",
    "    targets_all = []\n",
    "    \n",
    "    for images, labels in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        images, labels = images.to(device), labels.to(device).unsqueeze(1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.cuda.amp.autocast(enabled=(scaler is not None)):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "        if scaler:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        \n",
    "        preds_all.extend(torch.sigmoid(outputs).detach().cpu().numpy())\n",
    "        targets_all.extend(labels.detach().cpu().numpy())\n",
    "        \n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    # Binary metrics\n",
    "    preds_binary = (np.array(preds_all) > 0.5).astype(int)\n",
    "    acc = accuracy_score(targets_all, preds_binary)\n",
    "    \n",
    "    return epoch_loss, acc\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    preds_all = []\n",
    "    targets_all = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc=\"Validating\", leave=False):\n",
    "            images, labels = images.to(device), labels.to(device).unsqueeze(1)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            preds_all.extend(torch.sigmoid(outputs).cpu().numpy())\n",
    "            targets_all.extend(labels.cpu().numpy())\n",
    "            \n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    preds_binary = (np.array(preds_all) > 0.5).astype(int)\n",
    "    acc = accuracy_score(targets_all, preds_binary)\n",
    "    f1 = f1_score(targets_all, preds_binary)\n",
    "    try:\n",
    "        roc = roc_auc_score(targets_all, preds_all)\n",
    "    except:\n",
    "        roc = 0.5\n",
    "    \n",
    "    return epoch_loss, acc, f1, roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "model = get_model(CONFIG['model_name']).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=CONFIG['lr'])\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "best_loss = float('inf')\n",
    "save_path = f\"{CONFIG['model_dir']}/best_model_fold_{FOLD}_b2.pth\"\n",
    "\n",
    "print(f\"Starting Training for {CONFIG['epochs']} Epochs...\")\n",
    "\n",
    "for epoch in range(CONFIG['epochs']):\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device, scaler)\n",
    "    val_loss, val_acc, val_f1, val_roc = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{CONFIG['epochs']}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val   Loss: {val_loss:.4f}   | Acc: {val_acc:.4f} | F1: {val_f1:.4f} | ROC: {val_roc:.4f}\")\n",
    "    \n",
    "    if val_loss < best_loss:\n",
    "        print(f\"ðŸ”¥ Loss Improved ({best_loss:.4f} -> {val_loss:.4f}). Saving Model...\")\n",
    "        best_loss = val_loss\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "    print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
